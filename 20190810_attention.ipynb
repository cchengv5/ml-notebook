{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文档！！\n",
    "https://kexue.fm/archives/4765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if not np_load_old:\n",
    "    np_load_old = np.load\n",
    "    np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 100s 4ms/step - loss: 0.4181 - acc: 0.8051 - val_loss: 0.3606 - val_acc: 0.8388\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 0.2605 - acc: 0.8900 - val_loss: 0.3896 - val_acc: 0.8305\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 106s 4ms/step - loss: 0.1949 - acc: 0.9219 - val_loss: 0.4545 - val_acc: 0.8174\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 107s 4ms/step - loss: 0.2153 - acc: 0.9145 - val_loss: 0.4308 - val_acc: 0.8048\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 107s 4ms/step - loss: 0.2854 - acc: 0.8797 - val_loss: 0.5130 - val_acc: 0.7956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f609864efd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from attention_master.attention_keras import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "S_inputs = Input(shape=(None,), dtype='int32')\n",
    "embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "# embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "y_train shape: (25000,)\n",
      "x_test shape: (25000, 80)\n",
      "y_test shape: (25000,)\n",
      "new q_dense :  128\n",
      "new k_dense :  128\n",
      "new v_dense :  128\n",
      "q is : <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"embedding_9/embedding_lookup/Identity:0\", shape=(?, ?, 128), dtype=float32)\n",
      "k is : <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"embedding_9/embedding_lookup/Identity:0\", shape=(?, ?, 128), dtype=float32)\n",
      "v is : <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"embedding_9/embedding_lookup/Identity:0\", shape=(?, ?, 128), dtype=float32)\n",
      "layer.build (None, None, 128)\n",
      "layer.build (None, None, 128)\n",
      "layer.build (None, None, 128)\n",
      "qw is: Tensor(\"attention_9/Reshape_2:0\", shape=(?, ?, 128), dtype=float32)\n",
      "kw is: Tensor(\"attention_9/Reshape_5:0\", shape=(?, ?, 128), dtype=float32)\n",
      "vw is: Tensor(\"attention_9/Reshape_8:0\", shape=(?, ?, 128), dtype=float32)\n",
      "qw after reshap is: Tensor(\"attention_9/Reshape_9:0\", shape=(?, ?, 8, 16), dtype=float32)\n",
      "kw after reshap is: Tensor(\"attention_9/Reshape_10:0\", shape=(?, ?, 8, 16), dtype=float32)\n",
      "vw after reshap is: Tensor(\"attention_9/Reshape_11:0\", shape=(?, ?, 8, 16), dtype=float32)\n",
      "qw after permute_dimensions is: Tensor(\"attention_9/transpose_3:0\", shape=(?, 8, ?, 16), dtype=float32)\n",
      "kw after permute_dimensions is: Tensor(\"attention_9/transpose_4:0\", shape=(?, 8, ?, 16), dtype=float32)\n",
      "vw after permute_dimensions is: Tensor(\"attention_9/transpose_5:0\", shape=(?, 8, ?, 16), dtype=float32)\n",
      "1. a: Tensor(\"attention_9/truediv:0\", shape=(?, 8, ?, ?), dtype=float32)\n",
      "2. a: Tensor(\"attention_9/transpose_6:0\", shape=(?, ?, ?, 8), dtype=float32)\n",
      "3. a: Tensor(\"attention_9/transpose_6:0\", shape=(?, ?, ?, 8), dtype=float32)\n",
      "4. a: Tensor(\"attention_9/transpose_7:0\", shape=(?, 8, ?, ?), dtype=float32)\n",
      "5. a: Tensor(\"attention_9/sub_1:0\", shape=(?, 8, ?, ?), dtype=float32)\n",
      "6. a: Tensor(\"attention_9/Softmax:0\", shape=(?, 8, ?, ?), dtype=float32)\n",
      "1. o: Tensor(\"attention_9/MatMul_4:0\", shape=(?, 8, ?, 16), dtype=float32)\n",
      "2. o: Tensor(\"attention_9/transpose_8:0\", shape=(?, ?, 8, 16), dtype=float32)\n",
      "3. o: Tensor(\"attention_9/Reshape_12:0\", shape=(?, ?, 128), dtype=float32)\n",
      "4. o: Tensor(\"attention_9/Reshape_12:0\", shape=(?, ?, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from imp import reload\n",
    "\n",
    "from attention_master.attention_keras import Attention\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, GlobalAveragePooling1D, Dropout, Dense\n",
    "\n",
    "S_inputs = Input(shape=(None,), dtype='int32')\n",
    "embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "# embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True, to_file='xxx.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"xxx.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(url = 'xxx.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
